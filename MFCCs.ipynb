{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0bCZYvOesNaC"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import urllib.request\n",
        "import tarfile\n",
        "import torch\n",
        "import torchaudio\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKLWoZfqzonf",
        "outputId": "46049e67-3a87-4fd9-bd6d-99fef3ffce63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9iRp0_hkk_p",
        "outputId": "13cdbb1f-6708-4463-c3e7-d5e3ae2299a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Downloading Google Speech Commands dataset...\n",
            "Dataset info: https://arxiv.org/abs/1804.03209\n",
            "Downloading from: https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz\n",
            "Size: ~2GB - this may take several minutes...\n",
            "Download progress: 100.0%\n",
            "‚úÖ Download complete\n",
            "üì¶ Extracting dataset...\n",
            "‚úÖ Dataset extraction complete\n",
            "Found 36 command categories\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def download_speech_commands(data_path=\"./speech_commands_data\"):\n",
        "    \"\"\"\n",
        "    Download and extract Google Speech Commands dataset v0.02\n",
        "\n",
        "    Returns:\n",
        "        bool: True if successful, False otherwise\n",
        "    \"\"\"\n",
        "    data_path = Path(data_path)\n",
        "\n",
        "    if data_path.exists() and any(data_path.iterdir()):\n",
        "        print(\"‚úÖ Dataset already exists\")\n",
        "        return True\n",
        "\n",
        "    print(\"üì• Downloading Google Speech Commands dataset...\")\n",
        "    print(\"Dataset info: https://arxiv.org/abs/1804.03209\")\n",
        "\n",
        "    url = \"https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz\"\n",
        "    tar_path = data_path.parent / \"speech_commands_v0.02.tar.gz\"\n",
        "\n",
        "    try:\n",
        "        # Create directory\n",
        "        data_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Download with progress\n",
        "        print(f\"Downloading from: {url}\")\n",
        "        print(\"Size: ~2GB - this may take several minutes...\")\n",
        "\n",
        "        def progress_hook(block_num, block_size, total_size):\n",
        "            downloaded = block_num * block_size\n",
        "            if total_size > 0:\n",
        "                percent = min(100, downloaded * 100 / total_size)\n",
        "                print(f\"\\rDownload progress: {percent:.1f}%\", end=\"\", flush=True)\n",
        "\n",
        "        urllib.request.urlretrieve(url, tar_path, reporthook=progress_hook)\n",
        "        print(\"\\n‚úÖ Download complete\")\n",
        "\n",
        "        # Extract\n",
        "        print(\"üì¶ Extracting dataset...\")\n",
        "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "            tar.extractall(data_path)\n",
        "\n",
        "        # Cleanup\n",
        "        tar_path.unlink()\n",
        "        print(\"‚úÖ Dataset extraction complete\")\n",
        "\n",
        "        # Verify\n",
        "        commands_found = [d.name for d in data_path.iterdir() if d.is_dir()]\n",
        "        print(f\"Found {len(commands_found)} command categories\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error downloading dataset: {e}\")\n",
        "        print(\"\\nManual download instructions:\")\n",
        "        print(\n",
        "            \"1. Download: https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_v0.02.tar.gz\"\n",
        "        )\n",
        "        print(f\"2. Extract to: {data_path}\")\n",
        "        return False\n",
        "\n",
        "# Run the download and extraction process.\n",
        "download_speech_commands(data_path=\"./speech_commands_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyvTzjMsvG4U",
        "outputId": "4a4a835c-938a-4cb4-d698-9d352907c642"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of training files: 30769\n",
            "Number of validation files: 3703\n",
            "Number of testing files: 4074\n"
          ]
        }
      ],
      "source": [
        "# Define the 10 core commands\n",
        "commands = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\n",
        "\n",
        "# Define the data path\n",
        "data_path = Path(\"./speech_commands_data\")\n",
        "\n",
        "# Read validation and testing lists\n",
        "with open(data_path / 'validation_list.txt', 'r') as f:\n",
        "    val_rel_paths = set(f.read().splitlines())\n",
        "with open(data_path / 'testing_list.txt', 'r') as f:\n",
        "    test_rel_paths = set(f.read().splitlines())\n",
        "\n",
        "# Get all .wav files in the 10 command directories\n",
        "all_files = []\n",
        "for command in commands:\n",
        "    command_dir = data_path / command\n",
        "    wav_files = list(command_dir.glob('*.wav'))\n",
        "    all_files.extend(wav_files)\n",
        "\n",
        "# Assign files to train, val, test\n",
        "train_files = []\n",
        "val_files = []\n",
        "test_files = []\n",
        "for file in all_files:\n",
        "    rel_path = str(file.relative_to(data_path))\n",
        "    if rel_path in val_rel_paths:\n",
        "        val_files.append(file)\n",
        "    elif rel_path in test_rel_paths:\n",
        "        test_files.append(file)\n",
        "    else:\n",
        "        train_files.append(file)\n",
        "\n",
        "print(f\"Number of training files: {len(train_files)}\")\n",
        "print(f\"Number of validation files: {len(val_files)}\")\n",
        "print(f\"Number of testing files: {len(test_files)}\")\n",
        "\n",
        "# Define label map\n",
        "label_map = {command: i for i, command in enumerate(commands)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueyWEFcCAVYr",
        "outputId": "85c54c97-1702-48ac-8fcc-221e0af42e18"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing MFCCs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30769/30769 [03:12<00:00, 159.80it/s]\n",
            "Computing MFCCs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3703/3703 [00:22<00:00, 167.19it/s]\n",
            "Computing MFCCs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4074/4074 [00:24<00:00, 168.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MFCCs saved to Google Drive.\n"
          ]
        }
      ],
      "source": [
        "# Define MFCC transform\n",
        "mfcc_transform = torchaudio.transforms.MFCC(\n",
        "    sample_rate=16000,\n",
        "    n_mfcc=20,\n",
        "    melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': 40}\n",
        ")\n",
        "\n",
        "# Function to compute MFCCs\n",
        "def compute_mfccs(file_list, label_map, transform):\n",
        "    mfccs = []\n",
        "    labels = []\n",
        "    max_len = 0\n",
        "\n",
        "    # Compute all MFCCs and find the max sequence length\n",
        "    for file in tqdm(file_list, desc=\"Computing MFCCs\"):\n",
        "        waveform, sr = torchaudio.load(file)\n",
        "        if sr != 16000:\n",
        "            print(f\"Warning: sample rate {sr} for {file}\")\n",
        "        mfcc = transform(waveform).squeeze(0).transpose(0, 1)  # Shape: (seq_len, n_mfcc)\n",
        "        max_len = max(max_len, mfcc.shape[0])\n",
        "        mfccs.append(mfcc)\n",
        "        labels.append(label_map[file.parent.name])\n",
        "\n",
        "    # Pad all to max_len\n",
        "    padded_mfccs = []\n",
        "    for mfcc in mfccs:\n",
        "        padded = F.pad(mfcc, (0, 0, 0, max_len - mfcc.shape[0]))\n",
        "        padded_mfccs.append(padded)\n",
        "\n",
        "    mfccs_tensor = torch.stack(padded_mfccs)  # Shape: (num_samples, seq_len, n_mfcc)\n",
        "    labels_tensor = torch.tensor(labels)  # Shape: (num_samples,)\n",
        "    return mfccs_tensor, labels_tensor\n",
        "\n",
        "# Compute MFCCs for each split\n",
        "train_mfccs, train_labels = compute_mfccs(train_files, label_map, mfcc_transform)\n",
        "val_mfccs, val_labels = compute_mfccs(val_files, label_map, mfcc_transform)\n",
        "test_mfccs, test_labels = compute_mfccs(test_files, label_map, mfcc_transform)\n",
        "\n",
        "# Save to Google Drive\n",
        "save_path = Path('/content/drive/MyDrive/speech_commands_mfccs')\n",
        "save_path.mkdir(parents=True, exist_ok=True)\n",
        "torch.save({'mfccs': train_mfccs, 'labels': train_labels}, save_path / 'train.pt')\n",
        "torch.save({'mfccs': val_mfccs, 'labels': val_labels}, save_path / 'val.pt')\n",
        "torch.save({'mfccs': test_mfccs, 'labels': test_labels}, save_path / 'test.pt')\n",
        "print(\"MFCCs saved to Google Drive.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZZB0RxRGtWC"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
